{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0xs1d/pwskills/blob/main/Ensemble_Assignment_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73c02f1c",
      "metadata": {
        "id": "73c02f1c"
      },
      "source": [
        "# Ensemble Learning Assignment — Bagging, Random Forest, Boosting\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a302d96",
      "metadata": {
        "id": "1a302d96"
      },
      "source": [
        "## 1. Can we use Bagging for regression problems?\n",
        "Yes. Bagging works for both classification and regression. In regression, the final prediction is obtained by averaging predictions of all base regressors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd1e4ead",
      "metadata": {
        "id": "cd1e4ead"
      },
      "source": [
        "## 2. Difference between multiple model training and single model training\n",
        "Single model training uses one algorithm on the full dataset. Multiple model training trains several models on different subsets and aggregates their predictions to improve stability and accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bed0365e",
      "metadata": {
        "id": "bed0365e"
      },
      "source": [
        "## 3. Concept of feature randomness in Random Forest\n",
        "Random Forest selects a random subset of features at each split. This decorrelates trees and improves generalization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "709feb3d",
      "metadata": {
        "id": "709feb3d"
      },
      "source": [
        "## 4. What is OOB (Out-of-Bag) score?\n",
        "OOB score evaluates model performance using samples not included in the bootstrap sample. It acts like built‑in cross‑validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d796a728",
      "metadata": {
        "id": "d796a728"
      },
      "source": [
        "## 5. Measuring feature importance in Random Forest\n",
        "Feature importance is computed using Gini importance or permutation-based importance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdf1ce0b",
      "metadata": {
        "id": "cdf1ce0b"
      },
      "source": [
        "## 6. Working principle of a Bagging Classifier\n",
        "It trains multiple models on bootstrap samples and combines predictions using majority vote.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c6272e",
      "metadata": {
        "id": "b7c6272e"
      },
      "source": [
        "## 7. Evaluating a Bagging Classifier’s performance\n",
        "Use metrics such as accuracy, precision, recall, F1-score, or cross‑validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d082ad",
      "metadata": {
        "id": "79d082ad"
      },
      "source": [
        "## 8. How a Bagging Regressor works\n",
        "Trains multiple regressors on bootstrap samples and averages their predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40887e4c",
      "metadata": {
        "id": "40887e4c"
      },
      "source": [
        "## 9. Main advantage of ensemble techniques\n",
        "They reduce variance and improve predictive performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de906593",
      "metadata": {
        "id": "de906593"
      },
      "source": [
        "## 10. Main challenge of ensemble methods\n",
        "They can be computationally expensive and less interpretable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9419d691",
      "metadata": {
        "id": "9419d691"
      },
      "source": [
        "## 11. Key idea behind ensemble techniques\n",
        "Combine multiple diverse models to achieve better accuracy than a single model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f210c7be",
      "metadata": {
        "id": "f210c7be"
      },
      "source": [
        "## 12. What is a Random Forest Classifier?\n",
        "An ensemble of decision trees using bagging and random feature selection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2841a16d",
      "metadata": {
        "id": "2841a16d"
      },
      "source": [
        "## 13. Main types of ensemble techniques\n",
        "Bagging, Boosting, Stacking, Blending.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10332851",
      "metadata": {
        "id": "10332851"
      },
      "source": [
        "## 14. What is ensemble learning?\n",
        "Machine learning technique where multiple models work together to improve performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "568a1a37",
      "metadata": {
        "id": "568a1a37"
      },
      "source": [
        "## 15. When should we avoid using ensemble methods?\n",
        "When interpretability is required or computational resources are limited.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bec7d0ac",
      "metadata": {
        "id": "bec7d0ac"
      },
      "source": [
        "## 16. How Bagging helps reduce overfitting\n",
        "Bagging reduces variance by averaging predictions from diverse models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f490697",
      "metadata": {
        "id": "2f490697"
      },
      "source": [
        "## 17. Why Random Forest is better than a single Decision Tree\n",
        "It reduces overfitting and provides more stable predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88c1c1e8",
      "metadata": {
        "id": "88c1c1e8"
      },
      "source": [
        "## 18. Role of bootstrap sampling in Bagging\n",
        "It creates multiple diverse datasets, increasing model diversity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7582a6c",
      "metadata": {
        "id": "e7582a6c"
      },
      "source": [
        "## 19. Real-world applications of ensemble techniques\n",
        "Fraud detection, medical diagnosis, credit scoring, recommendation engines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db4b8755",
      "metadata": {
        "id": "db4b8755"
      },
      "source": [
        "## 20. Difference between Bagging and Boosting\n",
        "Bagging builds models independently in parallel; Boosting builds models sequentially, correcting previous errors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6004f8f2",
      "metadata": {
        "id": "6004f8f2"
      },
      "source": [
        "## Train a Bagging Classifier using Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "135a2b96",
      "metadata": {
        "id": "135a2b96"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_iris(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y,test_size=0.3,random_state=42)\n",
        "model = BaggingClassifier(DecisionTreeClassifier(), n_estimators=20)\n",
        "model.fit(Xtr,ytr)\n",
        "print(\"Accuracy:\", accuracy_score(yte, model.predict(Xte)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd59782c",
      "metadata": {
        "id": "fd59782c"
      },
      "source": [
        "## Train a Bagging Regressor and compute MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5538fe48",
      "metadata": {
        "id": "5538fe48"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_boston(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y,test_size=0.3)\n",
        "reg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=20)\n",
        "reg.fit(Xtr,ytr)\n",
        "print(\"MSE:\", mean_squared_error(yte, reg.predict(Xte)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2137ce1d",
      "metadata": {
        "id": "2137ce1d"
      },
      "source": [
        "## Random Forest Classifier — Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6a50d58",
      "metadata": {
        "id": "a6a50d58"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_breast_cancer(return_X_y=True)\n",
        "rf = RandomForestClassifier().fit(X,y)\n",
        "print(\"Feature Importance:\", rf.feature_importances_[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bdf8913",
      "metadata": {
        "id": "2bdf8913"
      },
      "source": [
        "## Random Forest Regressor vs Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5726cf0",
      "metadata": {
        "id": "b5726cf0"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_boston(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "dt = DecisionTreeRegressor().fit(Xtr,ytr)\n",
        "rf = RandomForestRegressor().fit(Xtr,ytr)\n",
        "print(\"DT MSE:\", mean_squared_error(yte, dt.predict(Xte)))\n",
        "print(\"RF MSE:\", mean_squared_error(yte, rf.predict(Xte)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43dfda20",
      "metadata": {
        "id": "43dfda20"
      },
      "source": [
        "## Compute Random Forest OOB Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "281152c4",
      "metadata": {
        "id": "281152c4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_breast_cancer(return_X_y=True)\n",
        "rf = RandomForestClassifier(oob_score=True, bootstrap=True).fit(X,y)\n",
        "print(\"OOB Score:\", rf.oob_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "237de45a",
      "metadata": {
        "id": "237de45a"
      },
      "source": [
        "## Bagging Classifier using SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "832083cb",
      "metadata": {
        "id": "832083cb"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_iris(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "model = BaggingClassifier(SVC(), n_estimators=10)\n",
        "model.fit(Xtr,ytr)\n",
        "print(\"Accuracy:\", model.score(Xte,yte))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50831c98",
      "metadata": {
        "id": "50831c98"
      },
      "source": [
        "## Random Forest with different n_estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a344c96",
      "metadata": {
        "id": "8a344c96"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_iris(return_X_y=True)\n",
        "for n in [10,50,100,200]:\n",
        "    print(n, RandomForestClassifier(n_estimators=n).fit(X,y).score(X,y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65ef5cbd",
      "metadata": {
        "id": "65ef5cbd"
      },
      "source": [
        "## Bagging + Logistic Regression — AUC Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65645b08",
      "metadata": {
        "id": "65645b08"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_breast_cancer(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "bag = BaggingClassifier(LogisticRegression(max_iter=500), n_estimators=10)\n",
        "bag.fit(Xtr,ytr)\n",
        "prob = bag.predict_proba(Xte)[:,1]\n",
        "print(\"AUC:\", roc_auc_score(yte, prob))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5975f125",
      "metadata": {
        "id": "5975f125"
      },
      "source": [
        "## Random Forest Regressor — Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05ebc1b5",
      "metadata": {
        "id": "05ebc1b5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_boston(return_X_y=True)\n",
        "rf = RandomForestRegressor().fit(X,y)\n",
        "print(rf.feature_importances_[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee57f730",
      "metadata": {
        "id": "ee57f730"
      },
      "source": [
        "## Bagging vs Random Forest — Accuracy Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1440cfe2",
      "metadata": {
        "id": "1440cfe2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_iris(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "bag = BaggingClassifier(DecisionTreeClassifier()).fit(Xtr,ytr)\n",
        "rf = RandomForestClassifier().fit(Xtr,ytr)\n",
        "print(\"Bagging:\", bag.score(Xte,yte))\n",
        "print(\"Random Forest:\", rf.score(Xte,yte))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f2b747c",
      "metadata": {
        "id": "9f2b747c"
      },
      "source": [
        "## Random Forest + GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b288393b",
      "metadata": {
        "id": "b288393b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_iris(return_X_y=True)\n",
        "params={'n_estimators':[50,100],'max_depth':[3,5,7]}\n",
        "grid = GridSearchCV(RandomForestClassifier(), params, cv=3).fit(X,y)\n",
        "print(\"Best Params:\", grid.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f59ad9",
      "metadata": {
        "id": "42f59ad9"
      },
      "source": [
        "## Bagging Regressor — Different Estimators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dff7604f",
      "metadata": {
        "id": "dff7604f"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_boston(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "for n in [5,10,20]:\n",
        "    br = BaggingRegressor(n_estimators=n).fit(Xtr,ytr)\n",
        "    print(n, mean_squared_error(yte, br.predict(Xte)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a0e29b3",
      "metadata": {
        "id": "5a0e29b3"
      },
      "source": [
        "## Random Forest — Misclassified Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b2914d",
      "metadata": {
        "id": "b3b2914d"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_iris(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "rf = RandomForestClassifier().fit(Xtr,ytr)\n",
        "pred = rf.predict(Xte)\n",
        "print(\"Misclassified:\", [(i, yte[i], pred[i]) for i in range(len(pred)) if pred[i]!=yte[i]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04e4bcb6",
      "metadata": {
        "id": "04e4bcb6"
      },
      "source": [
        "## Bagging Classifier vs Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d1cda80",
      "metadata": {
        "id": "7d1cda80"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_iris(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "dt = DecisionTreeClassifier().fit(Xtr,ytr)\n",
        "bag = BaggingClassifier(DecisionTreeClassifier()).fit(Xtr,ytr)\n",
        "print(\"DT:\", dt.score(Xte,yte))\n",
        "print(\"Bagging:\", bag.score(Xte,yte))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68fbf187",
      "metadata": {
        "id": "68fbf187"
      },
      "source": [
        "## Random Forest — Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e480820",
      "metadata": {
        "id": "5e480820"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_iris(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "rf = RandomForestClassifier().fit(Xtr,ytr)\n",
        "print(confusion_matrix(yte, rf.predict(Xte)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "328e8c1c",
      "metadata": {
        "id": "328e8c1c"
      },
      "source": [
        "## Stacking Classifier — DT + SVM + LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0737f5d4",
      "metadata": {
        "id": "0737f5d4"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_iris(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "estimators=[('dt',DecisionTreeClassifier()),('svm',SVC(probability=True))]\n",
        "stack = StackingClassifier(estimators, final_estimator=LogisticRegression())\n",
        "stack.fit(Xtr,ytr)\n",
        "print(\"Accuracy:\", stack.score(Xte,yte))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6b9d34e",
      "metadata": {
        "id": "f6b9d34e"
      },
      "source": [
        "## Random Forest — Top 5 Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f185ee",
      "metadata": {
        "id": "65f185ee"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_breast_cancer(return_X_y=True)\n",
        "rf = RandomForestClassifier().fit(X,y)\n",
        "fi = rf.feature_importances_\n",
        "idx = fi.argsort()[-5:][::-1]\n",
        "print(idx, fi[idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa436440",
      "metadata": {
        "id": "aa436440"
      },
      "source": [
        "## Bagging Classifier — Precision, Recall, F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d0d9176",
      "metadata": {
        "id": "2d0d9176"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_iris(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "bag = BaggingClassifier(DecisionTreeClassifier()).fit(Xtr,ytr)\n",
        "pred = bag.predict(Xte)\n",
        "print(\n",
        "    precision_score(yte,pred,average='weighted'),\n",
        "    recall_score(yte,pred,average='weighted'),\n",
        "    f1_score(yte,pred,average='weighted')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a85b913b",
      "metadata": {
        "id": "a85b913b"
      },
      "source": [
        "## Random Forest — Effect of max_depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cadf73c2",
      "metadata": {
        "id": "cadf73c2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_iris(return_X_y=True)\n",
        "for d in [2,4,6,8,10]:\n",
        "    rf = RandomForestClassifier(max_depth=d).fit(X,y)\n",
        "    print(d, rf.score(X,y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1876e2f3",
      "metadata": {
        "id": "1876e2f3"
      },
      "source": [
        "## Bagging Regressor — DT vs KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1ec44fe",
      "metadata": {
        "id": "b1ec44fe"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_boston(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "for base in [DecisionTreeRegressor(), KNeighborsRegressor()]:\n",
        "    br = BaggingRegressor(base).fit(Xtr,ytr)\n",
        "    print(type(base).__name__, mean_squared_error(yte, br.predict(Xte)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "439789e2",
      "metadata": {
        "id": "439789e2"
      },
      "source": [
        "## Random Forest — ROC-AUC Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c993795",
      "metadata": {
        "id": "5c993795"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_breast_cancer(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "rf = RandomForestClassifier().fit(Xtr,ytr)\n",
        "prob = rf.predict_proba(Xte)[:,1]\n",
        "print(\"ROC-AUC:\", roc_auc_score(yte, prob))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cd8d302",
      "metadata": {
        "id": "1cd8d302"
      },
      "source": [
        "## Bagging — Cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "080ca12c",
      "metadata": {
        "id": "080ca12c"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_iris(return_X_y=True)\n",
        "bag = BaggingClassifier(DecisionTreeClassifier())\n",
        "scores = cross_val_score(bag, X, y, cv=5)\n",
        "print(scores.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ea27192",
      "metadata": {
        "id": "1ea27192"
      },
      "source": [
        "## Random Forest — Precision-Recall curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64a2d003",
      "metadata": {
        "id": "64a2d003"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "X,y = load_breast_cancer(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "rf = RandomForestClassifier().fit(Xtr,ytr)\n",
        "prob = rf.predict_proba(Xte)[:,1]\n",
        "prec,rec,_ = precision_recall_curve(yte,prob)\n",
        "print(prec[:10], rec[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5b1fa38",
      "metadata": {
        "id": "c5b1fa38"
      },
      "source": [
        "## Stacking Classifier — RF + LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c412886",
      "metadata": {
        "id": "1c412886"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_iris(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "estimators=[('rf',RandomForestClassifier())]\n",
        "stack = StackingClassifier(estimators, final_estimator=LogisticRegression())\n",
        "stack.fit(Xtr,ytr)\n",
        "print(stack.score(Xte,yte))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcee6554",
      "metadata": {
        "id": "dcee6554"
      },
      "source": [
        "## Bagging Regressor — Different bootstrap sample sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aad83af5",
      "metadata": {
        "id": "aad83af5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine, load_boston\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
        "\n",
        "X,y = load_boston(return_X_y=True)\n",
        "Xtr,Xte,ytr,yte = train_test_split(X,y)\n",
        "for s in [0.4,0.6,1.0]:\n",
        "    br = BaggingRegressor(max_samples=s).fit(Xtr,ytr)\n",
        "    print(s, mean_squared_error(yte, br.predict(Xte)))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}